{
  "name": "@milosmiric/semantic-cache",
  "version": "1.0.0",
  "description": "Semantic caching for LLM queries using MongoDB Atlas Vector Search and VoyageAI embeddings",
  "type": "module",
  "main": "src/lib/index.ts",
  "module": "src/lib/index.ts",
  "types": "src/lib/index.ts",
  "bin": {
    "semantic-cache": "src/cli/index.ts"
  },
  "scripts": {
    "start": "bun run src/cli/index.ts",
    "cli": "bun run src/cli/index.ts",
    "demo": "bun run src/cli/index.ts demo",
    "query": "bun run src/cli/index.ts query",
    "stats": "bun run src/cli/index.ts stats",
    "clear": "bun run src/cli/index.ts clear",
    "test": "bun test",
    "test:coverage": "bun test --coverage",
    "typecheck": "tsc --noEmit"
  },
  "keywords": [
    "semantic-cache",
    "llm",
    "mongodb",
    "vector-search",
    "embeddings",
    "voyageai",
    "openai",
    "anthropic",
    "vercel-ai-sdk"
  ],
  "author": "Miloš Mirić",
  "license": "MIT",
  "devDependencies": {
    "@types/bun": "^1.3.3"
  },
  "peerDependencies": {
    "typescript": "^5"
  },
  "dependencies": {
    "@ai-sdk/openai": "^2.0.79",
    "ai": "^5.0.108",
    "chalk": "^5.6.2",
    "commander": "^14.0.2",
    "mongodb": "^7.0.0",
    "voyageai": "^0.0.8",
    "zod": "^4.1.13"
  }
}